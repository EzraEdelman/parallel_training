{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env JAX_CHECK_TRACER_LEAKS = 1\n",
    "from models.transformer import Transformer, TransformerConfig\n",
    "from models.mlp import MLP, MLPConfig\n",
    "from flax import nnx\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from functools import partial\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, List, Dict, Any, Callable\n",
    "import dataclasses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclasses.dataclass(unsafe_hash=True)\n",
    "class TrainConfig:\n",
    "  lrs: jax.Array\n",
    "  num_seeds: int\n",
    "  criterion: Callable\n",
    "  model: nnx.Module\n",
    "  model_config: Any = False\n",
    "\n",
    "  def replace(self, **kwargs):\n",
    "    return dataclasses.replace(self, **kwargs)\n",
    "\n",
    "\n",
    "def init_model(config: TrainConfig):\n",
    "  rngs = nnx.Rngs(params=0, dropout=1)\n",
    "  if not config.model_config:\n",
    "    config.model_config = config.model.config()\n",
    "  @nnx.jit\n",
    "  @nnx.split_rngs(splits=(config.num_seeds,))\n",
    "  @nnx.vmap(in_axes=(nnx.StateAxes({(nnx.Param,'params', 'dropout'): 0, ...: None}),None))\n",
    "  @nnx.vmap(in_axes=(None,0,))\n",
    "  def _init_model(rngs, lr):\n",
    "      model = config.model(config.model_config, rngs)\n",
    "      optimizer = nnx.Optimizer(model, optax.adam(lr), wrt=nnx.Param)\n",
    "\n",
    "      metrics = nnx.MultiMetric(\n",
    "        accuracy=nnx.metrics.Accuracy(threshold=None),\n",
    "        loss=nnx.metrics.Average('loss'),\n",
    "      )\n",
    "      return model, nnx.state(optimizer), metrics\n",
    "  return _init_model(rngs, config.lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=('criterion'))\n",
    "@nnx.vmap(in_axes=(0,0,0, 0, 0, None, None)) # data seeds #in_axes=(state_axes, nnx.StateAxes({nnx.Variable:0, ...: None}), 0)\n",
    "@nnx.vmap(in_axes=(0,0,0, None, 0,0, None)) # model params\n",
    "def _train_step(graphdef, state, metric_split, data, optimizer_states, lr,criterion):\n",
    "  model = nnx.merge(graphdef, state)\n",
    "  metrics = nnx.merge(*metric_split)\n",
    "  X, y = data\n",
    "  def loss_fn(model):\n",
    "    y_pred = model(X)[..., -1, :]\n",
    "    return criterion(y_pred, y), y_pred\n",
    "  (loss,logits), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n",
    "  metrics.update(logits=logits, loss=loss, labels=y)\n",
    "  temp = nnx.Optimizer(model, optax.adamw(lr), wrt=nnx.Param)\n",
    "  optimizer = nnx.merge(nnx.graphdef(temp), optimizer_states)\n",
    "  optimizer.update(model, grads)\n",
    "  return model, nnx.state(optimizer), metrics\n",
    "\n",
    "def train_step(models, data, optimizer_states, config, metrics):\n",
    "  models.train()\n",
    "  graphdef, state = nnx.split(models)\n",
    "  models, optimizer_states, metrics = _train_step(graphdef, state, nnx.split(metrics)  , data, optimizer_states, config.lrs, config.criterion)\n",
    "  # models = nnx.merge(graphdef, state)\n",
    "  # metrics.update(logits=logits, loss=loss, labels=y)\n",
    "  return models, optimizer_states, metrics\n",
    "\n",
    "@partial(jax.jit, static_argnames=('criterion'))\n",
    "@nnx.vmap(in_axes=(0,0, None, None)) # data seeds #in_axes=(state_axes, nnx.StateAxes({nnx.Variable:0, ...: None}), 0)\n",
    "@nnx.vmap(in_axes=(0,0, None, None)) # model params\n",
    "# @partial(jax.jit, static_argnames=('criterion', 'data'))\n",
    "def _eval_step(model, metric, data, criterion):\n",
    "  # model, metrics = nnx.merge(graphdef, state)\n",
    "  model = nnx.merge(*model)\n",
    "  metric = nnx.merge(*metric)\n",
    "  X, y = data\n",
    "  logits = model(X)[..., -1, :]\n",
    "  loss = criterion(logits, y)\n",
    "  metric.update(logits=logits, loss=loss, labels=y)\n",
    "  # (logits=logits, loss=loss, labels=y)\n",
    "  return metric\n",
    "\n",
    "def eval_step(models, data, config, metrics):\n",
    "  models.eval()\n",
    "  # graphdef, state = \n",
    "  metrics = _eval_step(nnx.split(models), nnx.split(metrics), data, config.criterion)\n",
    "  # metrics = nnx.merge(graphdef, state)\n",
    "  return metrics\n",
    "\n",
    "@nnx.jit\n",
    "@nnx.vmap\n",
    "@nnx.vmap\n",
    "def reset(metrics):\n",
    "    metrics.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(rng, n: int, d: int, k: int) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X_train : (n, d)\n",
    "      y_train : (n,)\n",
    "    \"\"\"\n",
    "    # rng = rng()\n",
    "    X   = jax.random.bernoulli(rng, p=0.5, shape=(n, d)).astype(jnp.int32)\n",
    "    # X = ((jax.random.choice(rng, 2**d, (n,), replace=False)[:, None] >> jnp.arange(d, dtype=jnp.uint32)) & 1).astype(jnp.float16)\n",
    "    # x = jax.random.choice(rng, 2**d, (n,), replace=False)\n",
    "    # shifts = jnp.arange(d - 1, -1, -1)\n",
    "    \n",
    "    # Add a new axis to input for broadcasting\n",
    "    # x_expanded = jnp.expand_dims(x, axis=-1)\n",
    "    \n",
    "    # Broadcast and perform bit operations in parallel\n",
    "    # X =  (x_expanded >> shifts) & 1\n",
    "    \n",
    "    # y   = - X[:, 0] * X[:, 1]\n",
    "    y = evaluate_parity(X, k)\n",
    "    return X, y\n",
    "\n",
    "def evaluate_parity(x: jnp.ndarray, k: int = 2) -> jnp.ndarray:\n",
    "    # return jnp.prod(x[..., :k], axis=-1, dtype=jnp.float32)\n",
    "    return jnp.sum(x[..., :k], axis=-1)% 2\n",
    "    # return nnx.one_hot(jnp.sum(x[..., :k], axis=-1)% 2, 2, dtype=jnp.int32)\n",
    "@partial(jax.jit, static_argnums=(1,2,3))\n",
    "# @nnx.split_rngs(splits=config.num_seeds)\n",
    "@nnx.vmap(in_axes=(0,None, None, None))\n",
    "def create_batches(rng, n: int, d: int, k: int) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    return create_batch(rng(), n, d, k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "d = 20\n",
    "k = 6\n",
    "\n",
    "\n",
    "model_config = TransformerConfig(\n",
    "    vocab_size=2,\n",
    "    max_len=d,\n",
    "    embd_dim=256,\n",
    "    num_heads=8,\n",
    "    mlp_dim=1024,\n",
    "    qkv_dim=256,\n",
    "    num_layers = 2\n",
    ")\n",
    "# model_config = MLPConfig(\n",
    "#     in_dim=d,\n",
    "#     out_dim=1,\n",
    "#     hidden_dim=32,\n",
    "#     hidden_layers=4\n",
    "# )\n",
    "\n",
    "config = TrainConfig(\n",
    "    lrs = jnp.geomspace(1e-4,1e-1,2),\n",
    "    num_seeds=20,\n",
    "    # criterion = lambda y_pred, y: optax.squared_error(y_pred.squeeze(-1), y).mean(),\n",
    "    criterion = lambda y_pred, y: optax.softmax_cross_entropy_with_integer_labels(y_pred, y).mean(),\n",
    "    model = Transformer,\n",
    "    model_config = model_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, optimizer_states, metrics = init_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rng = nnx.Rngs(0)\n",
    "backup = nnx.split_rngs(data_rng, splits=config.num_seeds)\n",
    "data = create_batches(data_rng, batch_size, d, k)\n",
    "models, optimizer_states, metrics = train_step(models, data, optimizer_states, config, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0aa70be30a240f082570ec0aa6ed314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Reset the metrics for the test set.\u001b[39;00m\n\u001b[32m     27\u001b[39m reset(metrics)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m metrics = \u001b[43meval_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m acc, loss = metrics.compute().values()\n\u001b[32m     30\u001b[39m metrics_history[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtest_loss\u001b[39m\u001b[33m'\u001b[39m].append(loss.mean(axis=\u001b[32m0\u001b[39m).min()) \u001b[38;5;66;03m# Record the metrics.\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36meval_step\u001b[39m\u001b[34m(models, data, config, metrics)\u001b[39m\n\u001b[32m     42\u001b[39m models.eval()\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# graphdef, state = \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m metrics = \u001b[43m_eval_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# metrics = nnx.merge(graphdef, state)\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/parallel_training/.venv/lib/python3.12/site-packages/jax/_src/pjit.py:263\u001b[39m, in \u001b[36m_cpp_pjit.<locals>.cache_miss\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.no_tracing.value:\n\u001b[32m    259\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info.fun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    260\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`jit`, but \u001b[39m\u001b[33m'\u001b[39m\u001b[33mno_tracing\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is set\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    262\u001b[39m (outs, out_flat, out_tree, args_flat, jaxpr,\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m  executable, pgle_profiler, const_args) = \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m     \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m maybe_fastpath_data = _get_fastpath_data(\n\u001b[32m    267\u001b[39m     executable, out_tree, args_flat, out_flat, jaxpr.effects, jaxpr.consts,\n\u001b[32m    268\u001b[39m     jit_info.abstracted_axes, pgle_profiler,\n\u001b[32m    269\u001b[39m     const_args)\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outs, maybe_fastpath_data, _need_to_rebuild_with_fdo(pgle_profiler)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/parallel_training/.venv/lib/python3.12/site-packages/jax/_src/pjit.py:146\u001b[39m, in \u001b[36m_python_pjit_helper\u001b[39m\u001b[34m(fun, jit_info, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m   args_flat = \u001b[38;5;28mmap\u001b[39m(core.full_lower, args_flat)\n\u001b[32m    145\u001b[39m   core.check_eval_args(args_flat)\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   out_flat, compiled, profiler, const_args = \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m      \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    149\u001b[39m   out_flat = jit_p.bind(*args_flat, **p.params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/parallel_training/.venv/lib/python3.12/site-packages/jax/_src/pjit.py:1600\u001b[39m, in \u001b[36m_pjit_call_impl_python\u001b[39m\u001b[34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, donated_invars, ctx_mesh, name, keep_unused, inline, compiler_options_kvs, *args)\u001b[39m\n\u001b[32m   1588\u001b[39m \u001b[38;5;66;03m# Passing mutable PGLE profile here since it should be extracted by JAXPR to\u001b[39;00m\n\u001b[32m   1589\u001b[39m \u001b[38;5;66;03m# initialize the fdo_profile compile option.\u001b[39;00m\n\u001b[32m   1590\u001b[39m computation = _resolve_and_lower(\n\u001b[32m   1591\u001b[39m     args, jaxpr=jaxpr, in_shardings=in_shardings,\n\u001b[32m   1592\u001b[39m     out_shardings=out_shardings, in_layouts=in_layouts,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1598\u001b[39m     compiler_options_kvs=compiler_options_kvs,\n\u001b[32m   1599\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1600\u001b[39m compiled = \u001b[43mcomputation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1602\u001b[39m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compiled._auto_spmd_lowering \u001b[38;5;129;01mand\u001b[39;00m config.enable_checks.value:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/parallel_training/.venv/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2527\u001b[39m, in \u001b[36mMeshComputation.compile\u001b[39m\u001b[34m(self, compiler_options, device_assignment)\u001b[39m\n\u001b[32m   2524\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compilation_device_list, (\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), xc.DeviceList))\n\u001b[32m   2526\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options_kvs \u001b[38;5;129;01mor\u001b[39;00m device_assignment:\n\u001b[32m-> \u001b[39m\u001b[32m2527\u001b[39m   executable = \u001b[43mUnloadedMeshExecutable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_hlo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2528\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_hlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2529\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2530\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdevice_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompilation_device_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2531\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compiler_options_kvs:\n\u001b[32m   2532\u001b[39m     \u001b[38;5;28mself\u001b[39m._executable = executable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/parallel_training/.venv/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:3073\u001b[39m, in \u001b[36mUnloadedMeshExecutable.from_hlo\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3070\u001b[39m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   3072\u001b[39m util.test_event(\u001b[33m\"\u001b[39m\u001b[33mpxla_cached_compilation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3073\u001b[39m xla_executable = \u001b[43m_cached_compilation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspmd_lowering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtuple_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_spmd_lowering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_prop_to_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_prop_to_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpmap_nreps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auto_spmd_lowering:\n\u001b[32m   3080\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/parallel_training/.venv/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2854\u001b[39m, in \u001b[36m_cached_compilation\u001b[39m\u001b[34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_kvs, pgle_profiler)\u001b[39m\n\u001b[32m   2846\u001b[39m compile_options = create_compile_options(\n\u001b[32m   2847\u001b[39m     computation, mesh, spmd_lowering, tuple_args, auto_spmd_lowering,\n\u001b[32m   2848\u001b[39m     allow_prop_to_inputs, allow_prop_to_outputs, backend,\n\u001b[32m   2849\u001b[39m     dev, pmap_nreps, compiler_options)\n\u001b[32m   2851\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dispatch.log_elapsed_time(\n\u001b[32m   2852\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{elapsed_time:.9f}\u001b[39;00m\u001b[33m sec\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2853\u001b[39m     fun_name=name, event=dispatch.BACKEND_COMPILE_EVENT):\n\u001b[32m-> \u001b[39m\u001b[32m2854\u001b[39m   xla_executable = \u001b[43mcompiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_or_get_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2855\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2856\u001b[39m \u001b[43m      \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2857\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/parallel_training/.venv/lib/python3.12/site-packages/jax/_src/compiler.py:491\u001b[39m, in \u001b[36mcompile_or_get_cached\u001b[39m\u001b[34m(backend, computation, devices, compile_options, host_callbacks, executable_devices, pgle_profiler)\u001b[39m\n\u001b[32m    489\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    490\u001b[39m   log_persistent_cache_miss(module_name, cache_key)\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_and_write_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m      \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m      \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/parallel_training/.venv/lib/python3.12/site-packages/jax/_src/compiler.py:759\u001b[39m, in \u001b[36m_compile_and_write_cache\u001b[39m\u001b[34m(backend, computation, executable_devices, compile_options, host_callbacks, module_name, cache_key)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compile_and_write_cache\u001b[39m(\n\u001b[32m    750\u001b[39m     backend: xc.Client,\n\u001b[32m    751\u001b[39m     computation: ir.Module,\n\u001b[32m   (...)\u001b[39m\u001b[32m    756\u001b[39m     cache_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    757\u001b[39m ) -> xc.LoadedExecutable:\n\u001b[32m    758\u001b[39m   start_time = time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m   executable = \u001b[43mbackend_compile_and_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    762\u001b[39m   compile_time = time.monotonic() - start_time\n\u001b[32m    763\u001b[39m   _cache_write(\n\u001b[32m    764\u001b[39m       cache_key, compile_time, module_name, backend, executable, host_callbacks\n\u001b[32m    765\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/parallel_training/.venv/lib/python3.12/site-packages/jax/_src/profiler.py:359\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    356\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    358\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/parallel_training/.venv/lib/python3.12/site-packages/jax/_src/compiler.py:375\u001b[39m, in \u001b[36mbackend_compile_and_load\u001b[39m\u001b[34m(backend, module, executable_devices, options, host_callbacks)\u001b[39m\n\u001b[32m    366\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m backend.compile_and_load(\n\u001b[32m    367\u001b[39m           built_c,\n\u001b[32m    368\u001b[39m           executable_devices=executable_devices,\n\u001b[32m    369\u001b[39m           compile_options=options,\n\u001b[32m    370\u001b[39m           host_callbacks=host_callbacks,\n\u001b[32m    371\u001b[39m       )\n\u001b[32m    372\u001b[39m     \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[32m    373\u001b[39m     \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[32m    374\u001b[39m     \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_and_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _jax.JaxRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    381\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m error_handler \u001b[38;5;129;01min\u001b[39;00m _XLA_RUNTIME_ERROR_HANDLERS:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "metrics_history = {\n",
    "  'train_loss': [],\n",
    "  'train_accuracy': [],\n",
    "  'test_loss': [],\n",
    "  'test_accuracy': [],\n",
    "}\n",
    "@nnx.jit\n",
    "@nnx.vmap\n",
    "def iterate_rngs(rngs):\n",
    "  rngs()\n",
    "num_steps = 2000\n",
    "data_rng = nnx.Rngs(0)\n",
    "backup = nnx.split_rngs(data_rng, splits=config.num_seeds)\n",
    "iterate_rngs(data_rng)\n",
    "# print(models.lm_head.kernel.value[1][0])\n",
    "test_data = create_batch(nnx.Rngs(-2)(), min(100, 2**d), d,   k)\n",
    "for step in tqdm(range(num_steps)):\n",
    "    data = create_batches(data_rng, batch_size, d, k)\n",
    "    iterate_rngs(data_rng)\n",
    "    models, optimizer_states, metrics = train_step(models, data, optimizer_states, config, metrics)\n",
    "    acc, loss = metrics.compute().values()\n",
    "    metrics_history[f'train_loss'].append(loss.mean(axis=0).min()) # Record the metrics.\n",
    "    metrics_history[f'train_accuracy'].append(acc.mean(axis=0).max()) # Record the metrics.\n",
    "    # Reset the metrics for the test set.\n",
    "    reset(metrics)\n",
    "    metrics = eval_step(models, test_data, config, metrics)\n",
    "    acc, loss = metrics.compute().values()\n",
    "    metrics_history[f'test_loss'].append(loss.mean(axis=0).min()) # Record the metrics.\n",
    "    metrics_history[f'test_accuracy'].append(jnp.mean(acc,axis=0).max()) # Record the metrics.\n",
    "      # Reset the metrics for the test set.\n",
    "    reset(metrics)\n",
    "    # for metric, value in metrics.compute().items():  # Compute the metrics.\n",
    "    #   metrics_history[f'test_{metric}'].append(value)  # Record the metrics.\n",
    "    # metrics.reset()  # Reset the metrics for the test set.\n",
    "    # if (step+1) % 30 == 0:\n",
    "    #   clear_output(wait=True)\n",
    "    #   fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    #   ax1.set_title('Loss')\n",
    "    #   ax2.set_title('Accuracy')\n",
    "    #   for dataset in ('train', 'test'):\n",
    "    #     ax1.plot(metrics_history[f'{dataset}_loss'], label=f'{dataset}_loss')\n",
    "    #     ax2.plot(metrics_history[f'{dataset}_accuracy'], label=f'{dataset}_accuracy')\n",
    "    #   ax1.legend()\n",
    "    #   ax2.legend()\n",
    "    #   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiheadAttention(\n",
      "  query_proj=Linear(\n",
      "    weight=f32[1,1,1], bias=None, in_features=1, out_features=1, use_bias=False\n",
      "  ),\n",
      "  key_proj=Linear(\n",
      "    weight=f32[1,1,1], bias=None, in_features=1, out_features=1, use_bias=False\n",
      "  ),\n",
      "  value_proj=Linear(\n",
      "    weight=f32[1,1,1], bias=None, in_features=1, out_features=1, use_bias=False\n",
      "  ),\n",
      "  output_proj=Linear(\n",
      "    weight=f32[1,1,1], bias=None, in_features=1, out_features=1, use_bias=False\n",
      "  ),\n",
      "  dropout=Dropout(p=None, inference=bool[1]),\n",
      "  num_heads=1,\n",
      "  query_size=1,\n",
      "  key_size=1,\n",
      "  value_size=1,\n",
      "  output_size=1,\n",
      "  qk_size=1,\n",
      "  vo_size=1,\n",
      "  use_query_bias=False,\n",
      "  use_key_bias=False,\n",
      "  use_value_bias=False,\n",
      "  use_output_bias=False\n",
      ")\n",
      "[False]\n",
      "False\n",
      "MultiheadAttention(\n",
      "  query_proj=Linear(\n",
      "    weight=f32[1,1,1], bias=None, in_features=1, out_features=1, use_bias=False\n",
      "  ),\n",
      "  key_proj=Linear(\n",
      "    weight=f32[1,1,1], bias=None, in_features=1, out_features=1, use_bias=False\n",
      "  ),\n",
      "  value_proj=Linear(\n",
      "    weight=f32[1,1,1], bias=None, in_features=1, out_features=1, use_bias=False\n",
      "  ),\n",
      "  output_proj=Linear(\n",
      "    weight=f32[1,1,1], bias=None, in_features=1, out_features=1, use_bias=False\n",
      "  ),\n",
      "  dropout=Dropout(p=None, inference=bool[1]),\n",
      "  num_heads=1,\n",
      "  query_size=1,\n",
      "  key_size=1,\n",
      "  value_size=1,\n",
      "  output_size=1,\n",
      "  qk_size=1,\n",
      "  vo_size=1,\n",
      "  use_query_bias=False,\n",
      "  use_key_bias=False,\n",
      "  use_value_bias=False,\n",
      "  use_output_bias=False\n",
      ")\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[183]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# print(isinstance(dropouts.inference, Sequence))\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(eqx.filter(dropouts, filt))\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43meqx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter_vmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynamic_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# no crash\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# eqx.filter_vmap(test)(dynamic_layers)\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# # crash\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/parallel_training/.venv/lib/python3.12/site-packages/equinox/_vmap_pmap.py:169\u001b[39m, in \u001b[36m_VmapWrapper.__call__\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    164\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    165\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCannot resolve batch dimension. Non-`None` `out_axes` requires \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    166\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33meither `in_axes` or `axis_size` to be not `None`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    167\u001b[39m         )\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m     vmapd, nonvmapd_arr, static = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_fun_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43min_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_axis_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_axis_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_vmapkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynamic_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m nonvmapd_static, out_axes = static.value\n\u001b[32m    179\u001b[39m nonvmapd = combine(nonvmapd_arr, nonvmapd_static)\n",
      "    \u001b[31m[... skipping hidden 8 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[183]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtest\u001b[39m\u001b[34m(temp)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mf\u001b[39m(_x, _y):\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/parallel_training/.venv/lib/python3.12/site-packages/jax/_src/lax/control_flow/loops.py:219\u001b[39m, in \u001b[36mscan\u001b[39m\u001b[34m(f, init, xs, length, reverse, unroll, _split_transpose)\u001b[39m\n\u001b[32m    216\u001b[39m xs_flat, xs_tree = tree_flatten(xs)\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m   lengths = [\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xs_flat]\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    221\u001b[39m   msg = \u001b[33m\"\u001b[39m\u001b[33mscan got value with no leading axis to scan over: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import equinox as eqx\n",
    "from jax import lax\n",
    "from jax import numpy as jnp\n",
    "import jax\n",
    "from typing import Sequence\n",
    "\n",
    "\n",
    "def test(temp):\n",
    "    def f(_x, _y):\n",
    "        return None,None\n",
    "    return lax.scan(f, None, temp)\n",
    "\n",
    "\n",
    "linear = lambda i: eqx.nn.Linear(1,1, key=jax.random.PRNGKey(0))\n",
    "linears = jax.vmap(linear)(jnp.array([0]))\n",
    "dynamic_layers, static_layers = eqx.partition(linears, eqx.is_array)\n",
    "# print(dynamic_layers)\n",
    "jax.vmap(test)(dynamic_layers)\n",
    "# no crash\n",
    "\n",
    "drop = eqx.nn.MultiheadAttention(1,1,1,1,1, key=jax.random.PRNGKey(0))\n",
    "def drops(i):\n",
    "    return drop\n",
    "dropouts = jax.vmap(lambda i: drop)(jnp.array([0]))\n",
    "def filt(x): # array and not eqx.nn.Dropout\n",
    "    return eqx.is_array(x) and (not isinstance(x, bool))  and (not x.weak_type)\n",
    "dynamic_layers, static_layers = eqx.partition(dropouts, filt)#, is_leaf=lambda x: isinstance(x, eqx.nn.Dropout))\n",
    "print(dynamic_layers)\n",
    "\n",
    "def check(n):\n",
    "    return isinstance(n, eqx.nn.Dropout)\n",
    "print(jax.vmap(check)(dropouts))\n",
    "print(check(linear))\n",
    "# print(isinstance(dropouts.inference, Sequence))\n",
    "print(eqx.filter(dropouts, filt))\n",
    "eqx.filter_vmap(test)(dynamic_layers)\n",
    "# no crash\n",
    "\n",
    "\n",
    "# def dropout(i):\n",
    "#     temp = eqx.nn.Dropout(0.5)\n",
    "#     return temp\n",
    "# dropouts = jax.vmap(dropout)(jnp.array([0]))\n",
    "\n",
    "# def part(x):\n",
    "#     return eqx.partition(x, eqx.is_array)\n",
    "# # dynamic_layers, static_layers = jax.vmap(part)(dropouts)\n",
    "# # print(dynamic_layers)\n",
    "# eqx.filter_vmap(test)(dynamic_layers)\n",
    "# # crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 32, 20)\n",
      "(20, 2, 10, 20, 2)\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 1 1 1 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "@nnx.vmap(in_axes=(0,None))\n",
    "@nnx.vmap(in_axes=(0, None))\n",
    "@nnx.jit\n",
    "def test(model, data):\n",
    "    return model(data)\n",
    "print(data[0].shape)\n",
    "data = create_batch(nnx.Rngs(-3)(), 10, d, k)\n",
    "print(test(models, data[0]).shape)\n",
    "print(jnp.argmax(test(models, data[0])[0,0,:,-1], axis=-1))\n",
    "print(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721ce305f068493e94096902d18dec29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def loss_fn(model, X,y):\n",
    "  y_pred = model(X)[..., -1, :]\n",
    "  return config.criterion(y_pred, y), y_pred\n",
    "@jax.jit\n",
    "@nnx.vmap(in_axes=(0,0,0, 0, 0, None)) # data seeds #in_axes=(state_axes, nnx.StateAxes({nnx.Variable:0, ...: None}), 0)\n",
    "@nnx.vmap(in_axes=(0,0,0, None, 0,0,)) # model params\n",
    "def _train_step2(graphdef, state, metric_split, data, optimizer_states, lr):\n",
    "  model = nnx.merge(graphdef, state)\n",
    "  # metrics = nnx.merge(*metric_split)\n",
    "  X, y = data\n",
    "  (loss,logits), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model, X,y)\n",
    "  # metrics.update(logits=logits, loss=loss, labels=y)\n",
    "  temp = nnx.Optimizer(model, optax.adamw(lr), wrt=nnx.Param)\n",
    "  optimizer = nnx.merge(nnx.graphdef(temp), optimizer_states)\n",
    "  optimizer.update(model, grads)\n",
    "  return nnx.state(model), nnx.state(optimizer), None#nnx.state(metrics)\n",
    "metrics_history = {\n",
    "  'train_loss': [],\n",
    "  'train_accuracy': [],\n",
    "  'test_loss': [],\n",
    "  'test_accuracy': [],\n",
    "}\n",
    "num_steps = 2000\n",
    "data_rng = nnx.Rngs(0)\n",
    "# backup = nnx.split_rngs(data_rng, splits=config.num_seeds)\n",
    "# print(models.lm_head.kernel.value[1][0])\n",
    "test_data = create_batch(nnx.Rngs(-2), min(1000, 2**d), d,   k)\n",
    "graphdef, state = nnx.split(models)\n",
    "metrics_graphdef, metrics_state = nnx.split(metrics)\n",
    "for step in tqdm(range(num_steps)):\n",
    "    data = create_batches(data_rng, batch_size, d, k)\n",
    "    # state, optimizer_states, metrics_state = _train_step2(graphdef, state, None, data, optimizer_states, config.lrs)\n",
    "    \n",
    "    # acc, loss = metrics.compute().values()\n",
    "    # metrics_history[f'train_loss'].append(loss.mean(axis=0).min()) # Record the metrics.\n",
    "    # metrics_history[f'train_accuracy'].append(acc.mean(axis=0).max()) # Record the metrics.\n",
    "    # # Reset the metrics for the test set.\n",
    "    # reset(metrics)\n",
    "    # metrics = eval_step(models, test_data, config, metrics)\n",
    "    # acc, loss = metrics.compute().values()\n",
    "    # metrics_history[f'test_loss'].append(loss.mean(axis=0).min()) # Record the metrics.\n",
    "    # metrics_history[f'test_accuracy'].append(jnp.mean(acc,axis=0).max()) # Record the metrics.\n",
    "    #   # Reset the metrics for the test set.\n",
    "    # reset(metrics)\n",
    "    # for metric, value in metrics.compute().items():  # Compute the metrics.\n",
    "    #   metrics_history[f'test_{metric}'].append(value)  # Record the metrics.\n",
    "    # metrics.reset()  # Reset the metrics for the test set.\n",
    "    # if (step+1) % 30 == 0:\n",
    "    #   clear_output(wait=True)\n",
    "    #   fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    #   ax1.set_title('Loss')\n",
    "    #   ax2.set_title('Accuracy')\n",
    "    #   for dataset in ('train'):\n",
    "    #     ax1.plot(metrics_history[f'{dataset}_loss'], label=f'{dataset}_loss')\n",
    "    #     ax2.plot(metrics_history[f'{dataset}_accuracy'], label=f'{dataset}_accuracy')\n",
    "    #   ax1.legend()\n",
    "    #   ax2.legend()\n",
    "    #   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Accuracy.update() missing 2 required keyword-only arguments: 'logits' and 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/parallel_training/.venv/lib/python3.12/site-packages/flax/nnx/training/metrics.py:414\u001b[39m, in \u001b[36mMultiMetric.update\u001b[39m\u001b[34m(self, **updates)\u001b[39m\n\u001b[32m    411\u001b[39m \u001b[38;5;66;03m# TODO: should we give the option of updating only some of the metrics and not all? e.g. if for some kwargs==None, don't do update\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[38;5;66;03m# TODO: should we raise an error if a kwarg is passed into **updates that has no match with any underlying metric? e.g. user typo\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m metric_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._metric_names:\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m   \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mupdates\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Accuracy.update() missing 2 required keyword-only arguments: 'logits' and 'labels'"
     ]
    }
   ],
   "source": [
    "metrics.update(train_loss=train_loss, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;79;201;177mRngKey\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n",
      "  \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray((), dtype=key<fry>) overlaying:\n",
      "  [0 0],\n",
      "  \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'params'\u001b[0m\n",
      "\u001b[38;2;255;213;3m)\u001b[0m\n",
      "\u001b[38;2;79;201;177mRngKey\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 2 (16 B)\u001b[0m\n",
      "  \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray((2,), dtype=key<fry>) overlaying:\n",
      "  [[4165894930  804218099]\n",
      "   [1353695780 2116000888]],\n",
      "  \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'params'\u001b[0m\n",
      "\u001b[38;2;255;213;3m)\u001b[0m\n",
      "\u001b[38;2;79;201;177mRngKey\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n",
      "  \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray((), dtype=key<fry>) overlaying:\n",
      "  [0 0],\n",
      "  \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'params'\u001b[0m\n",
      "\u001b[38;2;255;213;3m)\u001b[0m\n",
      "\u001b[38;2;79;201;177mRngKey\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 2 (16 B)\u001b[0m\n",
      "  \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray((2,), dtype=key<fry>) overlaying:\n",
      "  [[ 346279018  360566543]\n",
      "   [3968330031 3923691647]],\n",
      "  \u001b[38;2;156;220;254mtag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'params'\u001b[0m\n",
      "\u001b[38;2;255;213;3m)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "rngs = nnx.Rngs(params=0, dropout=1)\n",
    "temp = nnx.split_rngs(rngs, splits=2, only='params')\n",
    "print(rngs.params.key)\n",
    "nnx.restore_rngs(rngs)\n",
    "\n",
    "temp = nnx.split_rngs(rngs, splits=2, only='params')\n",
    "print(rngs.params.key)\n",
    "# print(temp[1].params)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
